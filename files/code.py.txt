import numpy as np

def calculate_metrics(confusion_matrix):
    # Number of classes
    num_classes = confusion_matrix.shape[0]
    
    # Initialize arrays to store precision, recall, and F1 scores for each class
    precision = np.zeros(num_classes)
    recall = np.zeros(num_classes)
    f1_score = np.zeros(num_classes)

    for i in range(num_classes):
        tp = confusion_matrix[i, i]  # True positives for class i
        fp = confusion_matrix[:, i].sum() - tp  # False positives for class i
        fn = confusion_matrix[i, :].sum() - tp  # False negatives for class i
        
        precision[i] = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall[i] = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1_score[i] = (2 * precision[i] * recall[i]) / (precision[i] + recall[i]) if (precision[i] + recall[i]) > 0 else 0
    
    return precision, recall, f1_score

def calculate_total_metrics(confusion_matrix):
    precision, recall, f1_score = calculate_metrics(confusion_matrix)
    
    # Calculate total precision, recall, and F1-score using the weighted approach
    support = confusion_matrix.sum(axis=1)  # Number of instances per class (support)
    total_precision = np.average(precision, weights=support)
    total_recall = np.average(recall, weights=support)
    total_f1_score = np.average(f1_score, weights=support)
    
    return total_precision, total_recall, total_f1_score

# Example confusion matrix for 5 classes
confusion_matrix = np.array([
    [30, 2, 1, 0, 2],  # Class 0
    [3, 25, 2, 0, 1],  # Class 1
    [1, 2, 27, 1, 0],  # Class 2
    [0, 0, 1, 28, 1],  # Class 3
    [1, 1, 0, 2, 26]   # Class 4
])

# Calculate total metrics
total_precision, total_recall, total_f1_score = calculate_total_metrics(confusion_matrix)

# Print total metrics
print("Total Precision:", total_precision)
print("Total Recall:", total_recall)
print("Total F1-Score:", total_f1_score)
